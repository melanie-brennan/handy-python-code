{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function calculate_binary_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_binary_metrics(true_labels, predicted_labels, print_results = True):\n",
    "    \"\"\"\n",
    "    Calculates a number of metrics - coinfusion table, accuracy, error rate, precision, recall, f1\n",
    "    true positive rate, true negative rate, false positive rate, false negative rate\n",
    "    :param true_labels: Ground truth labels\n",
    "    :param predicted_labels: Predicted labels\n",
    "    :param print_results: Boolean indicating whether metrics should be printed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate metrics...\n",
    "    # Get the number of labels\n",
    "    num_instances = true_labels.shape[0]\n",
    "\n",
    "    # Calculate confusion matrix - manually and with sklearn\n",
    "    # True positives - true label is 1 and predicted label is 1\n",
    "    tp_manual = np.sum(np.logical_and(true_labels==1, predicted_labels==1))\n",
    "    \n",
    "    # False_positive - true label is 0 but predicted label is 1\n",
    "    fp_manual = np.sum(np.logical_and(true_labels==0, predicted_labels==1))\n",
    "    \n",
    "    # True negative - true label is 0 and predicted label is 0\n",
    "    tn_manual = np.sum(np.logical_and(true_labels==0, predicted_labels==0))\n",
    "    \n",
    "    # False negative -true label is 1 but predicted label is 0\n",
    "    fn_manual = np.sum(np.logical_and(true_labels==1, predicted_labels==0))\n",
    "    \n",
    "    conf_matrix_manual = [[tn_manual, fp_manual],[fn_manual, tp_manual]]\n",
    "    \n",
    "    #Calculate confusion matrix with sklearn\n",
    "    conf_matrix_sklearn = confusion_matrix(true_labels, predicted_labels)\n",
    "    [[tn, fp],[fn, tp]] = conf_matrix_sklearn\n",
    "\n",
    "    # Use the sklearn values for tn, fn, fp, tp\n",
    "    # Convert to floats\n",
    "    (tn, fn, fp, tp) = (float(tn), float(fn), float(fp), float(tp))\n",
    "    \n",
    "    # Prevalence - the number of positive cases in the true labels divided by total number of instances\n",
    "    num_positive_cases = float(np.sum(true_labels==1))/ num_instances\n",
    "    \n",
    "    prevalence_manual = tp / (fp + fn + tp + tn)\n",
    "\n",
    "    # Calculate accuracy (normalised) - manually and with sklearn\n",
    "    accuracy_manual = (tp + tn) / (fp + fn + tp + tn)\n",
    "    acccuracy_sklearn = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "    # Calculate precison - manually and with sklearn\n",
    "    precision_manual = tp / (tp + fp)\n",
    "    precision_sklearn = precision_score(true_labels, predicted_labels)\n",
    "\n",
    "    # Calculate recall - manually and with sklearn\n",
    "    recall_manual = tp / (tp + fn)\n",
    "    recall_sklearn = recall_score(true_labels,predicted_labels)\n",
    "\n",
    "    # Calculate f1 - manually and with sklearn for confirmation\n",
    "    f1_manual1 = 2.0 * (precision_manual) * (recall_manual) / (precision_manual + recall_manual)\n",
    "    f1_manual2 = tp / (tp + ((fn+fp)/2.0))\n",
    "    f1_sklearn = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "    # Calculate error rate (misclassification rate)\n",
    "    error_rate_manual = (fp+fn) / (fp + fn + tp + tn)\n",
    "\n",
    "    # Calculate true positive rate (sensitivity) true negative rate (specificity)\n",
    "    tpr = tp /(tp + fn)\n",
    "    tnr = tn /(tn + fp)\n",
    "\n",
    "    #Calculate false positive rate and false negative rate\n",
    "    fpr = fp / (fp + tn)\n",
    "    fnr = fn / (fn + tp)\n",
    "\n",
    "    if print_results:\n",
    "        print(\"\\nNumber of labels to predict: \", num_instances)\n",
    "\n",
    "        print(\"\\nConfusion matrix, manual calculation...\")\n",
    "        print(conf_matrix_manual)\n",
    "        \n",
    "        print(\"Confusion matrix, sklearn...\")\n",
    "        print(conf_matrix_sklearn)\n",
    "        \n",
    "        print(\"\\ntn, fn, fp, tp values calculated by sklearn, converted to floats...\")\n",
    "        print(\"True negative:  \", tn)\n",
    "        print(\"False negative: \", fn)\n",
    "        print(\"False positive: \", fp)\n",
    "        print(\"True positive:  \", tp)\n",
    "        \n",
    "        print(\"\\nPrevalence, manual calculation: \", prevalence_manual)\n",
    "        \n",
    "        print(\"\\nNumber correct predictions: \", (tp + tn), \", out of \", (tp + tn + fn + fp))\n",
    "        print(\"\\nAccuracy normalised, manual calculation:  \", accuracy_manual)\n",
    "        print(\"Accuracy normalised, sklearn:             \", acccuracy_sklearn)\n",
    "\n",
    "        print(\"\\nError rate (misclassification rate), manual calculation: \", error_rate_manual)\n",
    "\n",
    "        print(\"\\nPrecision, manual calculation: \", precision_manual)\n",
    "        print(\"Precision, sklearn:            \", precision_sklearn)\n",
    "\n",
    "        print(\"\\nRecall, manual calculation: \", recall_manual)\n",
    "        print(\"Recall, sklearn:            \", recall_sklearn)\n",
    "\n",
    "        print(\"\\nf1, manual calculation method 1: \", f1_manual1)\n",
    "        print(\"f1, manual calculation method 2: \", f1_manual2)\n",
    "        print(\"f1, sklearn:                     \", f1_sklearn)\n",
    "\n",
    "        print(\"\\nTrue positive rate (sensitivity), manual calculation: \", tpr)\n",
    "        print(\"True negative rate (specificity), manual calculation: \", tnr)\n",
    "\n",
    "        print(\"\\nFalse positive rate, manual calculation: \", fpr)\n",
    "        print(\"False negative rate, manual calculation: \", fnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the function\n",
    "\n",
    "#### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of labels to predict:  4\n",
      "\n",
      "Confusion matrix, manual calculation...\n",
      "[[1, 1], [1, 1]]\n",
      "Confusion matrix, sklearn...\n",
      "[[1 1]\n",
      " [1 1]]\n",
      "\n",
      "tn, fn, fp, tp values calculated by sklearn, converted to floats...\n",
      "True negative:   1.0\n",
      "False negative:  1.0\n",
      "False positive:  1.0\n",
      "True positive:   1.0\n",
      "\n",
      "Prevalence, manual calculation:  0.25\n",
      "\n",
      "Number correct predictions:  2.0 , out of  4.0\n",
      "\n",
      "Accuracy normalised, manual calculation:   0.5\n",
      "Accuracy normalised, sklearn:              0.5\n",
      "\n",
      "Error rate (misclassification rate), manual calculation:  0.5\n",
      "\n",
      "Precision, manual calculation:  0.5\n",
      "Precision, sklearn:             0.5\n",
      "\n",
      "Recall, manual calculation:  0.5\n",
      "Recall, sklearn:             0.5\n",
      "\n",
      "f1, manual calculation method 1:  0.5\n",
      "f1, manual calculation method 2:  0.5\n",
      "f1, sklearn:                      0.5\n",
      "\n",
      "True positive rate (sensitivity), manual calculation:  0.5\n",
      "True negative rate (specificity), manual calculation:  0.5\n",
      "\n",
      "False positive rate, manual calculation:  0.5\n",
      "False negative rate, manual calculation:  0.5\n"
     ]
    }
   ],
   "source": [
    "# Create two binary arrays\n",
    "true_labels = np.array([1,1,0,0])\n",
    "predicted_labels = np.array([1,0,1,0])\n",
    "\n",
    "calculate_binary_metrics(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of labels to predict:  10\n",
      "\n",
      "Confusion matrix, manual calculation...\n",
      "[[4, 4], [1, 1]]\n",
      "Confusion matrix, sklearn...\n",
      "[[4 4]\n",
      " [1 1]]\n",
      "\n",
      "tn, fn, fp, tp values calculated by sklearn, converted to floats...\n",
      "True negative:   4.0\n",
      "False negative:  1.0\n",
      "False positive:  4.0\n",
      "True positive:   1.0\n",
      "\n",
      "Prevalence, manual calculation:  0.1\n",
      "\n",
      "Number correct predictions:  5.0 , out of  10.0\n",
      "\n",
      "Accuracy normalised, manual calculation:   0.5\n",
      "Accuracy normalised, sklearn:              0.5\n",
      "\n",
      "Error rate (misclassification rate), manual calculation:  0.5\n",
      "\n",
      "Precision, manual calculation:  0.2\n",
      "Precision, sklearn:             0.2\n",
      "\n",
      "Recall, manual calculation:  0.5\n",
      "Recall, sklearn:             0.5\n",
      "\n",
      "f1, manual calculation method 1:  0.28571428571428575\n",
      "f1, manual calculation method 2:  0.2857142857142857\n",
      "f1, sklearn:                      0.28571428571428575\n",
      "\n",
      "True positive rate (sensitivity), manual calculation:  0.5\n",
      "True negative rate (specificity), manual calculation:  0.5\n",
      "\n",
      "False positive rate, manual calculation:  0.5\n",
      "False negative rate, manual calculation:  0.5\n"
     ]
    }
   ],
   "source": [
    "# Create two binary arrays\n",
    "true_labels =      np.array([1,1,0,0,0,0,0,0,0,0])\n",
    "predicted_labels = np.array([1,0,1,0,1,0,1,0,1,0])\n",
    "\n",
    "calculate_binary_metrics(true_labels, predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
